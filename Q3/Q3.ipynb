{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in string.punctuation and token not in stop_words]\n",
    "    tokens = [token for token in tokens if token.strip() != '']\n",
    "    return tokens\n",
    "\n",
    "def create_positional_index(dataset_path):\n",
    "    positional_index = {}\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(dataset_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                tokens = preprocess_text(text)\n",
    "                for position, term in enumerate(tokens):\n",
    "                    if term not in positional_index:\n",
    "                        positional_index[term] = {}\n",
    "                    if filename not in positional_index[term]:\n",
    "                        positional_index[term][filename] = []\n",
    "                    positional_index[term][filename].append(position)\n",
    "    return positional_index\n",
    "\n",
    "def save_positional_index(positional_index, output_file):\n",
    "    with open(output_file, 'wb') as file:\n",
    "        pickle.dump(positional_index, file)\n",
    "\n",
    "def load_positional_index(pickle_file):\n",
    "    with open(pickle_file, 'rb') as file:\n",
    "        positional_index = pickle.load(file)\n",
    "    return positional_index\n",
    "\n",
    "dataset_path = \"text_files_preprocessed\"\n",
    "output_file = \"positional_index.pickle\"\n",
    "\n",
    "positional_index = create_positional_index(dataset_path)\n",
    "\n",
    "save_positional_index(positional_index, output_file)\n",
    "\n",
    "positional_index_loaded = load_positional_index(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'tension']\n",
      "['file1.txt', 'file103.txt', 'file106.txt', 'file110.txt', 'file111.txt', 'file115.txt', 'file118.txt', 'file13.txt', 'file137.txt', 'file141.txt', 'file143.txt', 'file154.txt', 'file155.txt', 'file157.txt', 'file159.txt', 'file16.txt', 'file160.txt', 'file162.txt', 'file163.txt', 'file164.txt', 'file166.txt', 'file172.txt', 'file174.txt', 'file175.txt', 'file176.txt', 'file179.txt', 'file18.txt', 'file189.txt', 'file19.txt', 'file2.txt', 'file204.txt', 'file207.txt', 'file210.txt', 'file217.txt', 'file220.txt', 'file234.txt', 'file235.txt', 'file240.txt', 'file245.txt', 'file252.txt', 'file254.txt', 'file265.txt', 'file274.txt', 'file277.txt', 'file28.txt', 'file282.txt', 'file288.txt', 'file29.txt', 'file292.txt', 'file293.txt', 'file299.txt', 'file30.txt', 'file304.txt', 'file305.txt', 'file311.txt', 'file316.txt', 'file321.txt', 'file325.txt', 'file332.txt', 'file338.txt', 'file342.txt', 'file347.txt', 'file354.txt', 'file355.txt', 'file358.txt', 'file362.txt', 'file367.txt', 'file37.txt', 'file378.txt', 'file390.txt', 'file393.txt', 'file396.txt', 'file4.txt', 'file40.txt', 'file400.txt', 'file404.txt', 'file407.txt', 'file413.txt', 'file422.txt', 'file43.txt', 'file44.txt', 'file455.txt', 'file46.txt', 'file471.txt', 'file493.txt', 'file497.txt', 'file499.txt', 'file501.txt', 'file503.txt', 'file513.txt', 'file522.txt', 'file525.txt', 'file526.txt', 'file532.txt', 'file536.txt', 'file539.txt', 'file541.txt', 'file544.txt', 'file549.txt', 'file551.txt', 'file571.txt', 'file575.txt', 'file58.txt', 'file584.txt', 'file585.txt', 'file586.txt', 'file595.txt', 'file597.txt', 'file598.txt', 'file603.txt', 'file610.txt', 'file619.txt', 'file624.txt', 'file625.txt', 'file626.txt', 'file628.txt', 'file634.txt', 'file637.txt', 'file645.txt', 'file65.txt', 'file655.txt', 'file656.txt', 'file660.txt', 'file674.txt', 'file676.txt', 'file678.txt', 'file680.txt', 'file681.txt', 'file689.txt', 'file703.txt', 'file704.txt', 'file711.txt', 'file713.txt', 'file715.txt', 'file716.txt', 'file72.txt', 'file720.txt', 'file722.txt', 'file731.txt', 'file736.txt', 'file739.txt', 'file748.txt', 'file755.txt', 'file765.txt', 'file769.txt', 'file77.txt', 'file770.txt', 'file772.txt', 'file775.txt', 'file777.txt', 'file78.txt', 'file782.txt', 'file786.txt', 'file793.txt', 'file794.txt', 'file795.txt', 'file8.txt', 'file800.txt', 'file807.txt', 'file813.txt', 'file815.txt', 'file816.txt', 'file819.txt', 'file821.txt', 'file827.txt', 'file841.txt', 'file843.txt', 'file844.txt', 'file85.txt', 'file850.txt', 'file857.txt', 'file859.txt', 'file860.txt', 'file864.txt', 'file867.txt', 'file870.txt', 'file883.txt', 'file885.txt', 'file888.txt', 'file890.txt', 'file896.txt', 'file9.txt', 'file90.txt', 'file907.txt', 'file908.txt', 'file911.txt', 'file912.txt', 'file913.txt', 'file917.txt', 'file918.txt', 'file919.txt', 'file924.txt', 'file925.txt', 'file931.txt', 'file938.txt', 'file941.txt', 'file942.txt', 'file944.txt', 'file96.txt', 'file974.txt', 'file976.txt', 'file983.txt', 'file984.txt', 'file991.txt']\n",
      "Number of documents retrieved for query 1 using positional index: 2\n",
      "Names of documents retrieved for query 1 using positional index: file143.txt, file1.txt\n"
     ]
    }
   ],
   "source": [
    "def preprocess_input(input_sequence):\n",
    "    preprocessed_sequence = []\n",
    "    for query in input_sequence:\n",
    "        preprocessed_query = preprocess_text(query)\n",
    "        preprocessed_sequence.append(preprocessed_query)\n",
    "    return preprocessed_sequence\n",
    "\n",
    "def evaluate_phrase_query(positional_index, query):\n",
    "    query_terms = query\n",
    "    documents_matching = []\n",
    "    for term_idx, term in enumerate(query_terms):\n",
    "        if term not in positional_index:\n",
    "            return []\n",
    "        term_postings = positional_index[term]\n",
    "        if term_idx == 0:\n",
    "            documents_matching = list(term_postings.keys())\n",
    "            print(documents_matching)\n",
    "        else:\n",
    "            documents_matching = [doc for doc in documents_matching if doc in term_postings.keys()]\n",
    "        \n",
    "        # for document in documents_matching:\n",
    "        #     for position in term_postings[document]:\n",
    "        #         if position + term_idx + 1 in term_postings[document]:\n",
    "        #             documents_matching.append(document)\n",
    "        #             break\n",
    "    return list(set(documents_matching))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    positional_index = load_positional_index(\"positional_index.pickle\")\n",
    "\n",
    "    N = int(input(\"Enter the number of queries: \"))\n",
    "    queries = []\n",
    "    for _ in range(N):\n",
    "        query = input().strip()\n",
    "        queries.append(query)\n",
    "\n",
    "    preprocessed_queries = preprocess_input(queries)\n",
    "\n",
    "    for idx, query in enumerate(queries):\n",
    "        print(preprocessed_queries[idx])\n",
    "        result = evaluate_phrase_query(positional_index, preprocessed_queries[idx])\n",
    "        print(f\"Number of documents retrieved for query {idx+1} using positional index: {len(result)}\")\n",
    "        print(f\"Names of documents retrieved for query {idx+1} using positional index: \" + \", \".join([f\"{doc}\" for doc in result]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'tension']\n",
      "No documents found for the term: good\n",
      "Number of documents retrieved for query 1 using positional index: 0\n",
      "Names of documents retrieved for query 1 using positional index: \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
