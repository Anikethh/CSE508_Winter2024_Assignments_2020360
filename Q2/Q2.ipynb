{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    tokens = [token for token in tokens if token.strip() != '']\n",
    "    return tokens\n",
    "\n",
    "def create_inverted_index(dataset_path):\n",
    "    inverted_index = {}\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(dataset_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                tokens = preprocess_text(text)\n",
    "                for position, term in enumerate(tokens):\n",
    "                    if term not in inverted_index:\n",
    "                        inverted_index[term] = []\n",
    "                    if filename not in inverted_index[term]:\n",
    "                        inverted_index[term].append(filename)\n",
    "    return inverted_index\n",
    "\n",
    "def save_inverted_index(inverted_index, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for term, documents in inverted_index.items():\n",
    "            file.write(term + \":\" + \",\".join(documents) + \"\\n\")\n",
    "\n",
    "dataset_path = \"text_files_preprocessed\"\n",
    "output_file = \"inverted_index.txt\"\n",
    "\n",
    "inverted_index = create_inverted_index(dataset_path)\n",
    "\n",
    "save_inverted_index(inverted_index, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_inverted_index_as_pickle(inverted_index, output_file):\n",
    "    with open(output_file, 'wb') as file:\n",
    "        pickle.dump(inverted_index, file)\n",
    "\n",
    "save_inverted_index_as_pickle(inverted_index, 'inverted_index.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 AND T2: ['file1.txt', 'file115.txt', 'file382.txt']\n",
      "T1 OR T2: ['file140.txt', 'file781.txt', 'file637.txt', 'file600.txt', 'file516.txt', 'file234.txt', 'file48.txt', 'file794.txt', 'file167.txt', 'file198.txt', 'file408.txt', 'file121.txt', 'file122.txt', 'file934.txt', 'file699.txt', 'file573.txt', 'file276.txt', 'file103.txt', 'file284.txt', 'file129.txt', 'file597.txt', 'file256.txt', 'file176.txt', 'file424.txt', 'file539.txt', 'file686.txt', 'file810.txt', 'file506.txt', 'file124.txt', 'file220.txt', 'file109.txt', 'file580.txt', 'file639.txt', 'file64.txt', 'file187.txt', 'file531.txt', 'file301.txt', 'file534.txt', 'file624.txt', 'file304.txt', 'file306.txt', 'file73.txt', 'file428.txt', 'file897.txt', 'file804.txt', 'file664.txt', 'file426.txt', 'file625.txt', 'file452.txt', 'file197.txt', 'file850.txt', 'file760.txt', 'file466.txt', 'file30.txt', 'file371.txt', 'file147.txt', 'file527.txt', 'file759.txt', 'file497.txt', 'file663.txt', 'file257.txt', 'file139.txt', 'file849.txt', 'file857.txt', 'file336.txt', 'file505.txt', 'file148.txt', 'file704.txt', 'file156.txt', 'file953.txt', 'file375.txt', 'file854.txt', 'file360.txt', 'file689.txt', 'file216.txt', 'file535.txt', 'file481.txt', 'file644.txt', 'file945.txt', 'file712.txt', 'file423.txt', 'file433.txt', 'file657.txt', 'file748.txt', 'file439.txt', 'file132.txt', 'file274.txt', 'file273.txt', 'file44.txt', 'file976.txt', 'file5.txt', 'file939.txt', 'file99.txt', 'file788.txt', 'file894.txt', 'file548.txt', 'file205.txt', 'file835.txt', 'file761.txt', 'file576.txt', 'file922.txt', 'file951.txt', 'file238.txt', 'file626.txt', 'file729.txt', 'file329.txt', 'file811.txt', 'file547.txt', 'file723.txt', 'file188.txt', 'file581.txt', 'file733.txt', 'file890.txt', 'file994.txt', 'file4.txt', 'file560.txt', 'file542.txt', 'file413.txt', 'file601.txt', 'file67.txt', 'file946.txt', 'file385.txt', 'file101.txt', 'file523.txt', 'file362.txt', 'file561.txt', 'file374.txt', 'file254.txt', 'file435.txt', 'file874.txt', 'file13.txt', 'file364.txt', 'file966.txt', 'file458.txt', 'file789.txt', 'file546.txt', 'file271.txt', 'file477.txt', 'file286.txt', 'file486.txt', 'file240.txt', 'file557.txt', 'file91.txt', 'file977.txt', 'file131.txt', 'file410.txt', 'file334.txt', 'file347.txt', 'file752.txt', 'file127.txt', 'file988.txt', 'file776.txt', 'file832.txt', 'file230.txt', 'file860.txt', 'file891.txt', 'file631.txt', 'file515.txt', 'file993.txt', 'file120.txt', 'file681.txt', 'file461.txt', 'file83.txt', 'file309.txt', 'file931.txt', 'file749.txt', 'file108.txt', 'file260.txt', 'file278.txt', 'file69.txt', 'file412.txt', 'file320.txt', 'file727.txt', 'file615.txt', 'file389.txt', 'file115.txt', 'file604.txt', 'file330.txt', 'file906.txt', 'file734.txt', 'file396.txt', 'file549.txt', 'file618.txt', 'file71.txt', 'file456.txt', 'file457.txt', 'file390.txt', 'file21.txt', 'file479.txt', 'file979.txt', 'file861.txt', 'file879.txt', 'file521.txt', 'file929.txt', 'file376.txt', 'file62.txt', 'file503.txt', 'file692.txt', 'file192.txt', 'file498.txt', 'file429.txt', 'file799.txt', 'file696.txt', 'file656.txt', 'file820.txt', 'file105.txt', 'file517.txt', 'file831.txt', 'file476.txt', 'file522.txt', 'file269.txt', 'file543.txt', 'file98.txt', 'file119.txt', 'file965.txt', 'file578.txt', 'file684.txt', 'file2.txt', 'file26.txt', 'file134.txt', 'file400.txt', 'file806.txt', 'file865.txt', 'file96.txt', 'file102.txt', 'file465.txt', 'file621.txt', 'file206.txt', 'file907.txt', 'file843.txt', 'file1.txt', 'file333.txt', 'file836.txt', 'file972.txt', 'file158.txt', 'file973.txt', 'file380.txt', 'file771.txt', 'file407.txt', 'file553.txt', 'file467.txt', 'file252.txt', 'file215.txt', 'file852.txt', 'file997.txt', 'file425.txt', 'file214.txt', 'file235.txt', 'file627.txt', 'file450.txt', 'file89.txt', 'file956.txt', 'file84.txt', 'file78.txt', 'file514.txt', 'file568.txt', 'file444.txt', 'file190.txt', 'file825.txt', 'file420.txt', 'file343.txt', 'file551.txt', 'file567.txt', 'file940.txt', 'file708.txt', 'file463.txt', 'file598.txt', 'file888.txt', 'file137.txt', 'file152.txt', 'file509.txt', 'file512.txt', 'file593.txt', 'file611.txt', 'file43.txt', 'file740.txt', 'file382.txt', 'file283.txt', 'file241.txt', 'file823.txt', 'file544.txt', 'file277.txt', 'file999.txt', 'file518.txt', 'file719.txt', 'file921.txt', 'file363.txt', 'file606.txt', 'file856.txt', 'file491.txt', 'file38.txt', 'file183.txt', 'file791.txt', 'file404.txt', 'file104.txt', 'file337.txt', 'file882.txt', 'file100.txt', 'file899.txt', 'file251.txt', 'file969.txt', 'file65.txt', 'file345.txt', 'file455.txt', 'file36.txt', 'file920.txt', 'file504.txt', 'file349.txt', 'file872.txt', 'file577.txt', 'file668.txt', 'file715.txt', 'file768.txt', 'file28.txt', 'file394.txt', 'file8.txt', 'file765.txt', 'file731.txt', 'file751.txt', 'file767.txt', 'file599.txt', 'file393.txt']\n",
      "T1 AND NOT T2: ['file140.txt', 'file776.txt', 'file781.txt', 'file832.txt', 'file637.txt', 'file600.txt', 'file230.txt', 'file860.txt', 'file891.txt', 'file631.txt', 'file515.txt', 'file993.txt', 'file516.txt', 'file120.txt', 'file681.txt', 'file461.txt', 'file234.txt', 'file48.txt', 'file794.txt', 'file83.txt', 'file309.txt', 'file931.txt', 'file167.txt', 'file198.txt', 'file408.txt', 'file121.txt', 'file122.txt', 'file934.txt', 'file749.txt', 'file699.txt', 'file108.txt', 'file260.txt', 'file278.txt', 'file69.txt', 'file412.txt', 'file320.txt', 'file727.txt', 'file615.txt', 'file573.txt', 'file389.txt', 'file276.txt', 'file604.txt', 'file330.txt', 'file906.txt', 'file734.txt', 'file103.txt', 'file396.txt', 'file284.txt', 'file129.txt', 'file549.txt', 'file618.txt', 'file597.txt', 'file71.txt', 'file456.txt', 'file457.txt', 'file256.txt', 'file176.txt', 'file424.txt', 'file539.txt', 'file686.txt', 'file810.txt', 'file506.txt', 'file390.txt', 'file124.txt', 'file220.txt', 'file21.txt', 'file109.txt', 'file479.txt', 'file979.txt', 'file580.txt', 'file639.txt', 'file64.txt', 'file861.txt', 'file187.txt', 'file879.txt', 'file531.txt', 'file301.txt', 'file534.txt', 'file624.txt', 'file929.txt', 'file376.txt', 'file304.txt', 'file62.txt', 'file503.txt', 'file306.txt', 'file692.txt', 'file192.txt', 'file498.txt', 'file73.txt', 'file429.txt', 'file428.txt', 'file799.txt', 'file696.txt', 'file897.txt', 'file804.txt', 'file664.txt', 'file656.txt', 'file426.txt', 'file820.txt', 'file625.txt', 'file105.txt', 'file517.txt', 'file831.txt', 'file476.txt', 'file522.txt', 'file452.txt', 'file197.txt', 'file850.txt', 'file269.txt', 'file543.txt', 'file98.txt', 'file119.txt', 'file965.txt', 'file760.txt', 'file578.txt', 'file684.txt', 'file466.txt', 'file30.txt', 'file2.txt', 'file26.txt', 'file134.txt', 'file371.txt', 'file400.txt', 'file806.txt', 'file147.txt', 'file527.txt', 'file865.txt', 'file96.txt', 'file102.txt', 'file465.txt', 'file497.txt', 'file621.txt', 'file206.txt', 'file907.txt', 'file843.txt', 'file663.txt', 'file257.txt', 'file139.txt', 'file849.txt', 'file857.txt', 'file336.txt', 'file333.txt', 'file505.txt', 'file836.txt', 'file148.txt', 'file972.txt', 'file704.txt', 'file158.txt', 'file156.txt', 'file953.txt', 'file973.txt', 'file375.txt', 'file854.txt', 'file380.txt', 'file771.txt', 'file407.txt', 'file360.txt', 'file553.txt', 'file467.txt', 'file252.txt', 'file215.txt', 'file852.txt', 'file689.txt', 'file216.txt', 'file997.txt', 'file535.txt', 'file481.txt', 'file425.txt', 'file214.txt', 'file235.txt', 'file644.txt', 'file945.txt', 'file712.txt', 'file423.txt', 'file627.txt', 'file433.txt', 'file450.txt', 'file657.txt', 'file89.txt', 'file748.txt', 'file956.txt', 'file439.txt', 'file132.txt', 'file84.txt', 'file274.txt', 'file273.txt', 'file44.txt', 'file78.txt', 'file514.txt', 'file976.txt', 'file5.txt', 'file939.txt', 'file99.txt', 'file788.txt', 'file894.txt', 'file568.txt', 'file444.txt', 'file190.txt', 'file825.txt', 'file548.txt', 'file205.txt', 'file835.txt', 'file420.txt', 'file343.txt', 'file551.txt', 'file567.txt', 'file940.txt', 'file708.txt', 'file463.txt', 'file761.txt', 'file598.txt', 'file576.txt', 'file888.txt', 'file137.txt', 'file152.txt', 'file509.txt', 'file512.txt', 'file593.txt', 'file611.txt', 'file922.txt', 'file43.txt', 'file740.txt', 'file951.txt', 'file238.txt', 'file283.txt', 'file626.txt', 'file241.txt', 'file823.txt', 'file729.txt', 'file544.txt', 'file329.txt', 'file811.txt', 'file547.txt', 'file723.txt', 'file277.txt', 'file188.txt', 'file581.txt', 'file999.txt', 'file518.txt', 'file733.txt', 'file890.txt', 'file994.txt', 'file4.txt', 'file560.txt', 'file719.txt', 'file542.txt', 'file921.txt', 'file363.txt', 'file606.txt', 'file856.txt', 'file491.txt', 'file38.txt', 'file413.txt', 'file183.txt', 'file601.txt', 'file67.txt', 'file791.txt', 'file404.txt', 'file104.txt', 'file337.txt', 'file882.txt', 'file946.txt', 'file385.txt', 'file101.txt', 'file523.txt', 'file362.txt', 'file561.txt', 'file374.txt', 'file254.txt', 'file100.txt', 'file899.txt', 'file251.txt', 'file969.txt', 'file65.txt', 'file435.txt', 'file345.txt', 'file874.txt', 'file455.txt', 'file13.txt', 'file36.txt', 'file920.txt', 'file504.txt', 'file349.txt', 'file364.txt', 'file872.txt', 'file966.txt', 'file458.txt', 'file789.txt', 'file546.txt', 'file271.txt', 'file477.txt', 'file577.txt', 'file286.txt', 'file486.txt', 'file668.txt', 'file240.txt', 'file715.txt', 'file557.txt', 'file768.txt', 'file28.txt', 'file394.txt', 'file91.txt', 'file977.txt', 'file131.txt', 'file8.txt', 'file765.txt', 'file410.txt', 'file334.txt', 'file347.txt', 'file752.txt', 'file731.txt', 'file751.txt', 'file767.txt', 'file127.txt', 'file599.txt', 'file988.txt', 'file393.txt']\n",
      "T1 OR NOT T2: ['file140.txt', 'file776.txt', 'file781.txt', 'file832.txt', 'file637.txt', 'file600.txt', 'file230.txt', 'file860.txt', 'file891.txt', 'file631.txt', 'file515.txt', 'file993.txt', 'file516.txt', 'file120.txt', 'file681.txt', 'file461.txt', 'file234.txt', 'file48.txt', 'file794.txt', 'file83.txt', 'file309.txt', 'file931.txt', 'file167.txt', 'file198.txt', 'file408.txt', 'file121.txt', 'file122.txt', 'file934.txt', 'file749.txt', 'file699.txt', 'file108.txt', 'file260.txt', 'file278.txt', 'file69.txt', 'file412.txt', 'file320.txt', 'file727.txt', 'file615.txt', 'file573.txt', 'file389.txt', 'file276.txt', 'file604.txt', 'file330.txt', 'file906.txt', 'file734.txt', 'file103.txt', 'file396.txt', 'file284.txt', 'file129.txt', 'file549.txt', 'file618.txt', 'file597.txt', 'file71.txt', 'file456.txt', 'file457.txt', 'file256.txt', 'file176.txt', 'file424.txt', 'file539.txt', 'file686.txt', 'file810.txt', 'file506.txt', 'file390.txt', 'file124.txt', 'file220.txt', 'file21.txt', 'file109.txt', 'file479.txt', 'file979.txt', 'file580.txt', 'file639.txt', 'file64.txt', 'file861.txt', 'file187.txt', 'file879.txt', 'file531.txt', 'file301.txt', 'file534.txt', 'file624.txt', 'file929.txt', 'file376.txt', 'file304.txt', 'file62.txt', 'file503.txt', 'file306.txt', 'file692.txt', 'file192.txt', 'file498.txt', 'file73.txt', 'file429.txt', 'file428.txt', 'file799.txt', 'file696.txt', 'file897.txt', 'file804.txt', 'file664.txt', 'file656.txt', 'file426.txt', 'file820.txt', 'file625.txt', 'file105.txt', 'file517.txt', 'file831.txt', 'file476.txt', 'file522.txt', 'file452.txt', 'file197.txt', 'file850.txt', 'file269.txt', 'file543.txt', 'file98.txt', 'file119.txt', 'file965.txt', 'file760.txt', 'file578.txt', 'file684.txt', 'file466.txt', 'file30.txt', 'file2.txt', 'file26.txt', 'file134.txt', 'file371.txt', 'file400.txt', 'file806.txt', 'file147.txt', 'file527.txt', 'file865.txt', 'file96.txt', 'file102.txt', 'file465.txt', 'file497.txt', 'file621.txt', 'file206.txt', 'file907.txt', 'file843.txt', 'file663.txt', 'file257.txt', 'file139.txt', 'file849.txt', 'file857.txt', 'file336.txt', 'file333.txt', 'file505.txt', 'file836.txt', 'file148.txt', 'file972.txt', 'file704.txt', 'file158.txt', 'file156.txt', 'file953.txt', 'file973.txt', 'file375.txt', 'file854.txt', 'file380.txt', 'file771.txt', 'file407.txt', 'file360.txt', 'file553.txt', 'file467.txt', 'file252.txt', 'file215.txt', 'file852.txt', 'file689.txt', 'file216.txt', 'file997.txt', 'file535.txt', 'file481.txt', 'file425.txt', 'file214.txt', 'file235.txt', 'file644.txt', 'file945.txt', 'file712.txt', 'file423.txt', 'file627.txt', 'file433.txt', 'file450.txt', 'file657.txt', 'file89.txt', 'file748.txt', 'file956.txt', 'file439.txt', 'file132.txt', 'file84.txt', 'file274.txt', 'file273.txt', 'file44.txt', 'file78.txt', 'file514.txt', 'file976.txt', 'file5.txt', 'file939.txt', 'file99.txt', 'file788.txt', 'file894.txt', 'file568.txt', 'file444.txt', 'file190.txt', 'file825.txt', 'file548.txt', 'file205.txt', 'file835.txt', 'file420.txt', 'file343.txt', 'file551.txt', 'file567.txt', 'file940.txt', 'file708.txt', 'file463.txt', 'file761.txt', 'file598.txt', 'file576.txt', 'file888.txt', 'file137.txt', 'file152.txt', 'file509.txt', 'file512.txt', 'file593.txt', 'file611.txt', 'file922.txt', 'file43.txt', 'file740.txt', 'file951.txt', 'file238.txt', 'file283.txt', 'file626.txt', 'file241.txt', 'file823.txt', 'file729.txt', 'file544.txt', 'file329.txt', 'file811.txt', 'file547.txt', 'file723.txt', 'file277.txt', 'file188.txt', 'file581.txt', 'file999.txt', 'file518.txt', 'file733.txt', 'file890.txt', 'file994.txt', 'file4.txt', 'file560.txt', 'file719.txt', 'file542.txt', 'file921.txt', 'file363.txt', 'file606.txt', 'file856.txt', 'file491.txt', 'file38.txt', 'file413.txt', 'file183.txt', 'file601.txt', 'file67.txt', 'file791.txt', 'file404.txt', 'file104.txt', 'file337.txt', 'file882.txt', 'file946.txt', 'file385.txt', 'file101.txt', 'file523.txt', 'file362.txt', 'file561.txt', 'file374.txt', 'file254.txt', 'file100.txt', 'file899.txt', 'file251.txt', 'file969.txt', 'file65.txt', 'file435.txt', 'file345.txt', 'file874.txt', 'file455.txt', 'file13.txt', 'file36.txt', 'file920.txt', 'file504.txt', 'file349.txt', 'file364.txt', 'file872.txt', 'file966.txt', 'file458.txt', 'file789.txt', 'file546.txt', 'file271.txt', 'file477.txt', 'file577.txt', 'file286.txt', 'file486.txt', 'file668.txt', 'file240.txt', 'file715.txt', 'file557.txt', 'file768.txt', 'file28.txt', 'file394.txt', 'file91.txt', 'file977.txt', 'file131.txt', 'file8.txt', 'file765.txt', 'file410.txt', 'file334.txt', 'file347.txt', 'file752.txt', 'file731.txt', 'file751.txt', 'file767.txt', 'file127.txt', 'file599.txt', 'file988.txt', 'file393.txt']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_inverted_index(pickle_file):\n",
    "    with open(pickle_file, 'rb') as file:\n",
    "        inverted_index = pickle.load(file)\n",
    "    return inverted_index\n",
    "\n",
    "def and_operation(inverted_index, term1, term2):\n",
    "    if term1 not in inverted_index or term2 not in inverted_index:\n",
    "        return []\n",
    "    return list(set(inverted_index[term1]) & set(inverted_index[term2]))\n",
    "\n",
    "def or_operation(inverted_index, term1, term2):\n",
    "    documents = set()\n",
    "    if term1 in inverted_index:\n",
    "        documents.update(inverted_index[term1])\n",
    "    if term2 in inverted_index:\n",
    "        documents.update(inverted_index[term2])\n",
    "    return list(documents)\n",
    "\n",
    "def and_not_operation(inverted_index, term1, term2):\n",
    "    if term1 not in inverted_index or term2 not in inverted_index:\n",
    "        return []\n",
    "    return list(set(inverted_index[term1]) - set(inverted_index[term2]))\n",
    "\n",
    "def or_not_operation(inverted_index, term1, term2):\n",
    "    documents = set(inverted_index.get(term1, []))\n",
    "    excluded_documents = set(inverted_index.get(term2, []))\n",
    "    return list(documents - excluded_documents)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inverted_index = load_inverted_index(\"inverted_index.pickle\")\n",
    "\n",
    "    term1 = \"great\"\n",
    "    term2 = \"stability\"\n",
    "\n",
    "    # T1 AND T2\n",
    "    print(\"T1 AND T2:\", and_operation(inverted_index, term1, term2))\n",
    "\n",
    "    # T1 OR T2\n",
    "    print(\"T1 OR T2:\", or_operation(inverted_index, term1, term2))\n",
    "\n",
    "    # T1 AND NOT T2\n",
    "    print(\"T1 AND NOT T2:\", and_not_operation(inverted_index, term1, term2))\n",
    "\n",
    "    # T1 OR NOT T2\n",
    "    print(\"T1 OR NOT T2:\", or_not_operation(inverted_index, term1, term2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: car OR bag AND NOT cannister\n",
      "Number of documents retrieved for query 1: 31\n",
      "Names of the documents retrieved for query 1: file698.txt, file981.txt, file738.txt, file73.txt, file264.txt, file459.txt, file863.txt, file892.txt, file573.txt, file174.txt, file3.txt, file942.txt, file860.txt, file404.txt, file699.txt, file886.txt, file682.txt, file746.txt, file780.txt, file864.txt, file665.txt, file166.txt, file797.txt, file542.txt, file313.txt, file686.txt, file956.txt, file118.txt, file466.txt, file363.txt, file930.txt\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in string.punctuation and token not in stop_words]\n",
    "    tokens = [token for token in tokens if token.strip() != '']\n",
    "    return tokens\n",
    "\n",
    "def load_inverted_index(pickle_file):\n",
    "    with open(pickle_file, 'rb') as file:\n",
    "        inverted_index = pickle.load(file)\n",
    "    return inverted_index\n",
    "\n",
    "def and_operation(inverted_index, documents, term):\n",
    "    if term not in inverted_index:\n",
    "        return []\n",
    "    return list(set(documents) & set(inverted_index[term]))\n",
    "\n",
    "def or_operation(inverted_index, documents, term):\n",
    "    if term in inverted_index:\n",
    "        documents.update(inverted_index[term])\n",
    "    return list(documents)\n",
    "\n",
    "def and_not_operation(inverted_index, documents, term):\n",
    "    if term not in inverted_index:\n",
    "        return documents\n",
    "    return list(set(documents) - set(inverted_index[term]))\n",
    "\n",
    "def or_not_operation(inverted_index, documents, term):\n",
    "    if term in inverted_index:\n",
    "        excluded_documents = set(inverted_index[term])\n",
    "    else:\n",
    "        excluded_documents = set()\n",
    "    return list(set(documents) - excluded_documents)\n",
    "\n",
    "\n",
    "def evaluate_query(inverted_index, query, operations):\n",
    "    query_terms = query.split()\n",
    "    if len(query_terms) != len(operations) + 1:\n",
    "        raise ValueError(\"Invalid query format\")\n",
    "\n",
    "    # Perform the first operation\n",
    "    operator = operations[0]\n",
    "    term1 = query_terms[0]\n",
    "    term2 = query_terms[1]\n",
    "    if operator == \"AND\":\n",
    "        if term1 not in inverted_index or term2 not in inverted_index:\n",
    "            result = []\n",
    "        else:\n",
    "            result = list(set(inverted_index[term1]) & set(inverted_index[term2]))\n",
    "    elif operator == \"OR\":\n",
    "        documents = set()\n",
    "        if term1 in inverted_index:\n",
    "            documents.update(inverted_index[term1])\n",
    "        if term2 in inverted_index:\n",
    "            documents.update(inverted_index[term2])\n",
    "        result = list(documents)\n",
    "    elif operator == \"AND NOT\":\n",
    "        if term1 not in inverted_index or term2 not in inverted_index:\n",
    "            result = []\n",
    "        else:\n",
    "            result = list(set(inverted_index[term1]) - set(inverted_index.get(term2, [])))\n",
    "    elif operator == \"OR NOT\":\n",
    "        if term1 not in inverted_index:\n",
    "            result = []\n",
    "        else:\n",
    "            documents = set(inverted_index[term1])\n",
    "            if term2 in inverted_index:\n",
    "                excluded_documents = set(inverted_index[term2])\n",
    "            else:\n",
    "                excluded_documents = set()\n",
    "            result = list(set(documents) - excluded_documents)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid operator: \" + operator)\n",
    "\n",
    "    # Perform subsequent operations\n",
    "    for i in range(1, len(operations)):\n",
    "        operator = operations[i]\n",
    "        next_term = query_terms[i + 1]\n",
    "        if operator == \"AND\":\n",
    "            result = list(set(result) & set(inverted_index.get(next_term, [])))\n",
    "        elif operator == \"OR\":\n",
    "            documents = set(result)\n",
    "            if next_term in inverted_index:\n",
    "                documents.update(inverted_index[next_term])\n",
    "            result = list(documents)\n",
    "        elif operator == \"AND NOT\":\n",
    "            result = list(set(result) - set(inverted_index.get(next_term, [])))\n",
    "        elif operator == \"OR NOT\":\n",
    "            documents = set(result)\n",
    "            if next_term in inverted_index:\n",
    "                excluded_documents = set(inverted_index[next_term])\n",
    "            else:\n",
    "                excluded_documents = set()\n",
    "            result = list(set(documents) - excluded_documents)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid operator: \" + operator)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_input(input_sequence):\n",
    "    preprocessed_sequence = []\n",
    "    for query in input_sequence:\n",
    "        preprocessed_query = preprocess_text(query)\n",
    "        preprocessed_sequence.append(\" \".join(preprocessed_query))\n",
    "    return preprocessed_sequence\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inverted_index = load_inverted_index(\"inverted_index.pickle\")\n",
    "\n",
    "    N = int(input(\"Enter the number of queries: \"))\n",
    "    queries = []\n",
    "    for _ in range(N):\n",
    "        query = input().strip()\n",
    "        operations = input().strip().split(\", \")\n",
    "        queries.append((query, operations))\n",
    "\n",
    "    preprocessed_queries = preprocess_input([query for query, _ in queries])\n",
    "\n",
    "    for idx, (query, operations) in enumerate(queries):\n",
    "        query_with_ops = \"\"\n",
    "        for token, op in zip(preprocessed_queries[idx].split(), operations):\n",
    "            query_with_ops += token + \" \" + op + \" \"\n",
    "        query_with_ops += preprocessed_queries[idx].split()[-1]\n",
    "        result = evaluate_query(inverted_index, preprocessed_queries[idx], operations)\n",
    "        print(f\"Query {idx+1}: {query_with_ops}\")\n",
    "        print(f\"Number of documents retrieved for query {idx+1}: {len(result)}\")\n",
    "        print(f\"Names of the documents retrieved for query {idx+1}: \" + \", \".join([f\"{i}\" for i in result]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    tokens = [token for token in tokens if token.strip() != '']\n",
    "    return tokens\n",
    "\n",
    "# Function to load positional index from file\n",
    "def load_positional_index(pickle_file):\n",
    "    with open(pickle_file, 'rb') as file:\n",
    "        positional_index = pickle.load(file)\n",
    "    return positional_index\n",
    "\n",
    "# Function to retrieve documents for phrase query using positional index\n",
    "def retrieve_documents_for_phrase_query(positional_index, phrase_query):\n",
    "    terms = preprocess_text(phrase_query)\n",
    "    if len(terms) == 0:\n",
    "        return []\n",
    "\n",
    "    # Initialize result with documents containing the first term\n",
    "    result = set(positional_index.get(terms[0], {}).keys())\n",
    "\n",
    "    # Iterate through terms in the phrase query\n",
    "    for term in terms[1:]:\n",
    "        if term in positional_index:\n",
    "            # Get documents containing the current term\n",
    "            documents_with_term = set(positional_index[term].keys())\n",
    "            # Find documents common with the current result\n",
    "            result = result.intersection(documents_with_term)\n",
    "\n",
    "    # Filter documents where phrase query appears\n",
    "    for document in list(result):\n",
    "        positions = [positional_index[term][document] for term in terms]\n",
    "        if not any(check_sequence(positions, i) for i in range(len(positions[0]))):\n",
    "            result.remove(document)\n",
    "\n",
    "    return list(result)\n",
    "\n",
    "# Function to check if a sequence of positions is consecutive\n",
    "def check_sequence(positions, index):\n",
    "    return all(positions[i][0] + index in positions[i+1] for i in range(len(positions)-1))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load positional index\n",
    "    positional_index = load_positional_index(\"positional_index.pickle\")\n",
    "\n",
    "    # Input\n",
    "    N = int(input(\"Enter the number of queries: \"))\n",
    "    queries = []\n",
    "    for _ in range(N):\n",
    "        query = input().strip()\n",
    "        queries.append(query)\n",
    "\n",
    "    # Preprocess queries\n",
    "    preprocessed_queries = [preprocess_text(query) for query in queries]\n",
    "\n",
    "    # Retrieve documents for each query using positional index\n",
    "    for idx, query in enumerate(preprocessed_queries):\n",
    "        result = retrieve_documents_for_phrase_query(positional_index, query)\n",
    "        print(f\"Number of documents retrieved for query {idx+1} using positional index: {len(result)}\")\n",
    "        print(f\"Names of documents retrieved for query {idx+1} using positional index: \" + \", \".join([f\"file{i}.txt\" for i in result]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
