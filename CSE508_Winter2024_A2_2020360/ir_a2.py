# -*- coding: utf-8 -*-
"""IR A2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qGD9jmtTkdupxlPd0GZyhPS4KpiJjLFl

## Imports
"""

from google.colab import drive
drive.mount('/content/gdrive/')

import torch
import csv
import pandas as pd
import gspread
import pickle
# from google.colab import auth
# from google.auth import default
import requests
import matplotlib.pyplot as plt

# creds, _ = default()
# gc = gspread.authorize(creds)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

# sheet_url = 'https://docs.google.com/spreadsheets/d/1tG1DAzHKkGhHo5-F5WosUU-P7bYPQ9tGeDFKBSX3UkE/edit#gid=654644383'
# sh = gc.open_by_url(sheet_url)
# rows = sh.sheet1.get_all_values()
# df = pd.DataFrame.from_records(rows)

csv_path = '/content/gdrive/MyDrive/IR A2/A2_Data - A2_Data.csv'
df = pd.read_csv(csv_path)

df.head()

for i, index, row in enumerate(df.iterrows()):
    id = row[0]
    image_url = row[1]
    text_description = row[2]

    print("ID: ", id)
    print("Image URL:", image_url)
    print("Text Description:", text_description)
    break

"""## Data Collection"""

import os
from io import BytesIO
from PIL import Image
import requests
import pickle

folder_path = '/content/gdrive/My Drive/IR A2/Images'

if not os.path.exists(folder_path):
    os.makedirs(folder_path)

failed_indexes = []

for index, row in df.iloc[1:].iterrows():
    image_url_str = row[1]

    image_url_list = eval(image_url_str)

    entry_folder_path = os.path.join(folder_path, f"entry_{index}")
    if not os.path.exists(entry_folder_path):
        os.makedirs(entry_folder_path)

    for i, image_url in enumerate(image_url_list):
        try:
            response = requests.get(image_url)
            if response.status_code == 200:
                img = Image.open(BytesIO(response.content))

                img.save(os.path.join(entry_folder_path, f"image_{i}.jpg"))
                print(f"Entry {index}: Image {i} saved successfully.")
            else:
                print(f"Entry {index}: Image {i} could not be downloaded.")
                failed_indexes.append(index)
        except Exception as e:
            print(f"Entry {index}: Image {i} could not be downloaded. Error: {e}")
            failed_indexes.append(index)

with open('/content/gdrive/My Drive/IR A2/failed_indexes.pkl', 'wb') as f:
    pickle.dump(failed_indexes, f)

print(failed_indexes)

import os
import pandas as pd

folder_path = '/content/gdrive/My Drive/IR A2/Images'

for index, row in df.iterrows():
    image_url_str = row[1]

    image_url_list = eval(image_url_str)

    entry_folder_path = os.path.join(folder_path, f"entry_{index+1}")

    for i, image_url in enumerate(image_url_list):
        with open(os.path.join(entry_folder_path, f'image_url_{i}.txt'), 'w') as file:
            file.write(image_url)

print("Image URLs written to text files.")

# import os

# folder_path = '/content/gdrive/My Drive/IR A2/Images'

# for index, row in df.iterrows():
#     entry_folder_path = os.path.join(folder_path, f"entry_{index}")

#     if not os.path.exists(entry_folder_path):
#         continue

#     for file_name in os.listdir(entry_folder_path):
#         if file_name.startswith('image_url'):
#             os.remove(os.path.join(entry_folder_path, file_name))

# print("Files deleted.")

import os
from io import BytesIO
from PIL import Image
import requests
import pickle

folder_path = '/content/gdrive/My Drive/IR A2/Images'

for index, row in df.iloc[1:].iterrows():
    id = row[0]

    entry_folder_path = os.path.join(folder_path, f"entry_{index}")

    with open(os.path.join(entry_folder_path, 'id.txt'), 'w') as file:
      file.write(str(id))

import os
import pickle

folder_path = '/content/gdrive/My Drive/IR A2/Images'

for index, row in df.iterrows():
    text = row[2]

    entry_folder_path = os.path.join(folder_path, f"entry_{index+1}")

    with open(os.path.join(entry_folder_path, 'review.txt'), 'w') as file:
      file.write(str(text))

"""## Feature Extraction"""

folder_path = '/content/gdrive/My Drive/IR A2/Images'

import pickle

with open('/content/gdrive/MyDrive/IR A2/failed_indexes.pkl', 'rb') as f:
    failed_indexes = pickle.load(f)

print("Loaded failed_indexes:", failed_indexes)

import torch
import torchvision.transforms as transforms
import torchvision.models as models
from PIL import Image
import matplotlib.pyplot as plt

# Define image transformations and normalization
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load and preprocess an example image
img_path = '/content/gdrive/MyDrive/IR A2/Images/entry_1/image_0.jpg'
img = Image.open(img_path)

# Show the original image
plt.figure(figsize=(6, 6))
plt.subplot(1, 2, 1)
plt.imshow(img)
plt.title('Original Image')
plt.axis('off')

# Apply transformations and show the transformed image
transformed_img = preprocess(img)
plt.subplot(1, 2, 2)
plt.imshow(transformed_img.permute(1, 2, 0))
plt.title('Transformed Image')
plt.axis('off')

plt.show()

import os
import torch
import torchvision.transforms as transforms
import torchvision.models as models
from PIL import Image
import pickle

# Load pre-trained MobileNetV2 model
model = models.mobilenet_v2(pretrained=True)
model.eval()  # Set model to evaluation mode

# Define image transformations and normalization
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Directory containing the entry folders
base_dir = '/content/gdrive/MyDrive/IR A2/Images/'

# Iterate over the entry folders
for entry_folder in os.listdir(base_dir):
    entry_folder_path = os.path.join(base_dir, entry_folder)
    if not os.path.isdir(entry_folder_path):
        continue

    features_dict = {}

    # Iterate over the images in the entry folder
    for image_name in os.listdir(entry_folder_path):
        if not image_name.endswith('.jpg'):
            continue

        image_path = os.path.join(entry_folder_path, image_name)

        # Load and preprocess the image
        img = Image.open(image_path)
        img_tensor = preprocess(img).unsqueeze(0)  # Add batch dimension

        # Extract features using MobileNetV2
        with torch.no_grad():
            features = model(img_tensor)

        # Normalize the extracted features
        normalized_features = torch.nn.functional.normalize(features.squeeze(0), p=2, dim=0)

        # Save the normalized features to the features dictionary
        features_dict[image_name] = normalized_features

    # Pickle dump the features dictionary
    features_pickle_path = os.path.join(entry_folder_path, 'features.pkl')
    with open(features_pickle_path, 'wb') as f:
        pickle.dump(features_dict, f)

    print(f"Features extracted and pickled for {entry_folder_path}")

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
import pandas as pd
import os
import pickle
import math

# Define a function for text preprocessing
def preprocess_text(text):
    text = text.lower()
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word.isalnum()]
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return tokens

# Directory containing the entry folders
base_dir = '/content/gdrive/MyDrive/IR A2/Images/'

# Collect the corpus and preprocess text
corpus = []
invalid_indexes = []
for index, row in df.iterrows():
    text_description = row[2]
    if not isinstance(text_description, str):
        print(f"Text description at index {index} is not a string:", text_description)
        invalid_indexes.append(index)
        text_description = 'NaN'  # Replace invalid input with 'NaN'
    tokens = preprocess_text(text_description)
    corpus.append(tokens)

# Calculate TF-IDF scores for each entry
tfidf_scores = []
for doc in corpus:
    tf_scores = {}
    for term in doc:
        tf_scores[term] = tf_scores.get(term, 0) + 1  # Term Frequency
    for term in tf_scores:
        tf_scores[term] = tf_scores[term] / len(doc)  # Normalize TF
    idf_scores = {}
    for term in set(doc):
        doc_freq = sum(1 for doc in corpus if term in doc)
        idf_scores[term] = math.log(len(corpus) / (1 + doc_freq))  # Inverse Document Frequency
    tfidf_scores.append({term: tf_scores[term] * idf_scores[term] for term in tf_scores})

# Save the TF-IDF scores to pickle files
for index, tfidf in enumerate(tfidf_scores):
    entry_folder_index = index + 1  # Entry folder index starts from 1
    entry_folder_path = os.path.join(base_dir, f"entry_{entry_folder_index}")
    if not os.path.exists(entry_folder_path):
        os.makedirs(entry_folder_path)

    # Pickle dump the TF-IDF scores
    tfidf_path = os.path.join(entry_folder_path, 'tf-idf.pkl')
    with open(tfidf_path, 'wb') as f:
        pickle.dump(tfidf, f)

    print(f"TF-IDF scores pickled for entry_{entry_folder_index}")

print("Indexes with invalid values:", invalid_indexes)

import pickle

with open('/content/gdrive/MyDrive/IR A2/corpus.pkl', 'wb') as f:
    pickle.dump(corpus, f)

with open('/content/gdrive/MyDrive/IR A2/tfidf_scores.pkl', 'wb') as f:
    pickle.dump(tfidf_scores, f)

tfidf_scores[0]

print(corpus[999])
print(index)

invalid_indexes

with open('/content/gdrive/MyDrive/IR A2/Images/entry_1000/text_features.pkl', 'rb') as f:
    text_features = pickle.load(f)

print(text_features)

with open('/content/gdrive/MyDrive/IR A2/Images/entry_1000/tfidf.pkl', 'rb') as f:
    tfidf = pickle.load(f)

# Print the loaded list
print("Tfidf score:", tfidf['product'])

"""## Retrieval"""

# import nltk
# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('wordnet')

import os
import torch
import torchvision.transforms as transforms
import torchvision.models as models
from PIL import Image
import pickle
from io import BytesIO
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import pandas as pd
import math
import string

model = models.mobilenet_v2(pretrained=True)
model.eval()

preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

def preprocess_text(text):
    text = text.lower()
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word.isalnum()]
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return tokens

with open('/content/gdrive/MyDrive/IR A2/corpus.pkl', 'rb') as f:
    corpus = pickle.load(f)

with open('/content/gdrive/MyDrive/IR A2/tfidf_scores.pkl', 'rb') as f:
    tfidf_scores = pickle.load(f)

def add_review_to_corpus(input_review, corpus):
    preprocessed_input_review = preprocess_text(input_review)

    corpus.append(preprocessed_input_review)

    tf_scores = {}
    for term in preprocessed_input_review:
        tf_scores[term] = tf_scores.get(term, 0) + 1  # Term Frequency
    for term in tf_scores:
        tf_scores[term] = tf_scores[term] / len(preprocessed_input_review)  # Normalize TF
    idf_scores = {}
    for term in set(preprocessed_input_review):
        doc_freq = sum(1 for doc in corpus if term in doc)
        idf_scores[term] = math.log(len(corpus) / (1 + doc_freq))  # Inverse Document Frequency
    tfidf_scores.append({term: tf_scores[term] * idf_scores[term] for term in tf_scores})

    input_tfidf_scores = tfidf_scores[-1]

    return input_tfidf_scores

def calculate_tfidf_similarity(doc_tfidf_scores, input_tfidf_scores):
    dot_product = sum(doc_tfidf_scores.get(term, 0) * input_tfidf_scores.get(term, 0) for term in set(doc_tfidf_scores) | set(input_tfidf_scores))
    doc_norm = math.sqrt(sum(score ** 2 for score in doc_tfidf_scores.values()))
    input_norm = math.sqrt(sum(score ** 2 for score in input_tfidf_scores.values()))
    cosine_similarity = dot_product / (doc_norm * input_norm)

    return cosine_similarity

model = model.to(device)

"""### Image Retrieval"""

with open('/content/gdrive/MyDrive/IR A2/corpus.pkl', 'rb') as f:
  corpus = pickle.load(f)

with open('/content/gdrive/MyDrive/IR A2/tfidf_scores.pkl', 'rb') as f:
  tfidf_scores = pickle.load(f)

from io import BytesIO
from tqdm import tqdm

base_dir = '/content/gdrive/MyDrive/IR A2/Images/'

input_image_path = '/content/gdrive/MyDrive/IR A2/Images/entry_1/image_0.jpg'
image_url = 'https://images-na.ssl-images-amazon.com/images/I/81q5+IxFVUL._SY88.jpg'
input_review = "Loving these vintage springs on my vintage strat. They have a good tension and great stability. If you are floating your bridge and want the most out of your springs than these are the way to go."
preprocessed_input_review = preprocess_text(input_review)

try:
  response = requests.get(image_url)
  if response.status_code == 200:
      input_img = Image.open(BytesIO(response.content))

      print(f"Image downloaded successfully.")
  else:
      print(f"Image could not be downloaded.")
except Exception as e:
  print(f"Image could not be downloaded. Error: {e}")

# input_img = Image.open(input_image_path)
input_img_tensor = preprocess(input_img).unsqueeze(0)

with torch.no_grad():
    input_features = model(input_img_tensor)

input_normalized_features = torch.nn.functional.normalize(input_features.squeeze(0), p=2, dim=0)

input_tfidf_scores = add_review_to_corpus(input_review, corpus)

similarities = []
for entry_folder in tqdm(os.listdir(base_dir)):
    entry_folder_path = os.path.join(base_dir, entry_folder)
    if not os.path.isdir(entry_folder_path):
        continue

    features_path = os.path.join(entry_folder_path, 'features.pkl')

    review_path = os.path.join(entry_folder_path, 'review.txt')
    review = [line.strip() for line in open(review_path)]

    if os.path.exists(features_path):
        with open(features_path, 'rb') as f:
            features_dict = pickle.load(f)

        max_similarity = 0
        for image_name, features in features_dict.items():
            similarity = cosine_similarity(input_normalized_features.reshape(1, -1), features.reshape(1, -1))[0][0]
            img_url_path = os.path.join(entry_folder_path, f"image_url_{image_name.split('_')[1].split('.')[0]}.txt")
            if similarity > max_similarity:
              max_similarity = similarity
              url = [line.strip() for line in open(img_url_path)]

        similarities.append((entry_folder, image_name, max_similarity, url, review))

similarities.sort(key=lambda x: x[2], reverse=True)

top_similar_images = similarities[:3]

# plt.figure(figsize=(6, 6))
# plt.subplot(1, 2, 1)
# plt.imshow(input_img)
# plt.title('Original Image')
# plt.axis('off')

# print(input_review)

for idx, (entry_folder, image_name, max_similarity, url, review) in enumerate(top_similar_images):
    # print(f"Top {idx+1} - Folder: {entry_folder}, Image: {image_name}, Image URL: {url}, Similarity: {similarity}")
    entry_folder_path = os.path.join(base_dir, entry_folder)

    id_path = os.path.join(entry_folder_path, 'id.txt')
    with open(id_path, 'r') as file:
        id = file.read()

    tfidf_score_path = os.path.join(entry_folder_path, 'tf-idf.pkl')
    with open(tfidf_score_path, 'rb') as f:
        tfidf_scores = pickle.load(f)
    text_similarity = calculate_tfidf_similarity(tfidf_scores, input_tfidf_scores)

    print(f"\nTop {idx+1} - Image URL: {url}")
    print(f"ID: {id}")
    print(f"Review: {review}")
    print(f"Image Similarity: {max_similarity}")
    print(f"Textual Similarity: {text_similarity}")
    print(f"Composite Similarity: {(max_similarity + text_similarity)/2}")

"""### Text Retrieval"""

with open('/content/gdrive/MyDrive/IR A2/corpus.pkl', 'rb') as f:
  corpus = pickle.load(f)

with open('/content/gdrive/MyDrive/IR A2/tfidf_scores.pkl', 'rb') as f:
  tfidf_scores = pickle.load(f)

from io import BytesIO
from tqdm import tqdm

base_dir = '/content/gdrive/MyDrive/IR A2/Images/'

input_image_path = '/content/gdrive/MyDrive/IR A2/Images/entry_1/image_0.jpg'
image_url = 'https://images-na.ssl-images-amazon.com/images/I/81q5+IxFVUL._SY88.jpg'
input_review = "Loving these vintage springs on my vintage strat. They have a good tension and great stability. If you are floating your bridge and want the most out of your springs than these are the way to go."
preprocessed_input_review = preprocess_text(input_review)

try:
  response = requests.get(image_url)
  if response.status_code == 200:
      input_img = Image.open(BytesIO(response.content))

      print(f"Image downloaded successfully.")
  else:
      print(f"Image could not be downloaded.")
except Exception as e:
  print(f"Image could not be downloaded. Error: {e}")

# input_img = Image.open(input_image_path)
input_img_tensor = preprocess(input_img).unsqueeze(0)

with torch.no_grad():
    input_features = model(input_img_tensor)

input_normalized_features = torch.nn.functional.normalize(input_features.squeeze(0), p=2, dim=0)

input_tfidf_scores = add_review_to_corpus(input_review, corpus)

similarities = []
for entry_folder in tqdm(os.listdir(base_dir)):
    entry_folder_path = os.path.join(base_dir, entry_folder)
    if not os.path.isdir(entry_folder_path):
        continue

    tfidf_score_path = os.path.join(entry_folder_path, 'tf-idf.pkl')
    with open(tfidf_score_path, 'rb') as f:
      tfidf_scores = pickle.load(f)
    text_similarity = calculate_tfidf_similarity(tfidf_scores, input_tfidf_scores)

    review_path = os.path.join(entry_folder_path, 'review.txt')
    review = [line.strip() for line in open(review_path)]

    similarities.append((entry_folder, text_similarity, review))

similarities.sort(key=lambda x: x[1], reverse=True)

top_similar_reviews = similarities[:3]

for idx, (entry_folder, text_similarity, review) in enumerate(top_similar_reviews):
    # print(f"Top {idx+1} - Folder: {entry_folder}, Image: {image_name}, Image URL: {url}, Similarity: {similarity}")
    entry_folder_path = os.path.join(base_dir, entry_folder)
    features_path = os.path.join(entry_folder_path, 'features.pkl')

    id_path = os.path.join(entry_folder_path, 'id.txt')
    with open(id_path, 'r') as file:
        id = file.read()

    with open(features_path, 'rb') as f:
        features_dict = pickle.load(f)

    max_similarity = 0

    for image_name, features in features_dict.items():
        similarity = cosine_similarity(input_normalized_features.reshape(1, -1), features.reshape(1, -1))[0][0]
        img_url_path = os.path.join(entry_folder_path, f"image_url_{image_name.split('_')[1].split('.')[0]}.txt")
        if similarity > max_similarity:
            max_similarity = similarity
            url = [line.strip() for line in open(img_url_path)]

    print(f"\nTop {idx+1} - Image URL: {url}")
    print(f"ID: {id}")
    print(f"Review: {review}")
    print(f"Image Similarity: {max_similarity}")
    print(f"Textual Similarity: {text_similarity}")
    print(f"Composite Similarity: {(max_similarity + text_similarity)/2}")

"""### Combine Retrieval"""

with open('/content/gdrive/MyDrive/IR A2/corpus.pkl', 'rb') as f:
  corpus = pickle.load(f)

with open('/content/gdrive/MyDrive/IR A2/tfidf_scores.pkl', 'rb') as f:
  tfidf_scores = pickle.load(f)

from io import BytesIO
from tqdm import tqdm

base_dir = '/content/gdrive/MyDrive/IR A2/Images/'

input_image_path = '/content/gdrive/MyDrive/IR A2/Images/entry_1/image_0.jpg'
image_url = 'https://images-na.ssl-images-amazon.com/images/I/81q5+IxFVUL._SY88.jpg'
input_review = "Loving these vintage springs on my vintage strat. They have a good tension and great stability. If you are floating your bridge and want the most out of your springs than these are the way to go."
preprocessed_input_review = preprocess_text(input_review)

try:
  response = requests.get(image_url)
  if response.status_code == 200:
      input_img = Image.open(BytesIO(response.content))

      print(f"Image downloaded successfully.")
  else:
      print(f"Image could not be downloaded.")
except Exception as e:
  print(f"Image could not be downloaded. Error: {e}")

# input_img = Image.open(input_image_path)
input_img_tensor = preprocess(input_img).unsqueeze(0)

with torch.no_grad():
    input_features = model(input_img_tensor)

input_normalized_features = torch.nn.functional.normalize(input_features.squeeze(0), p=2, dim=0)

input_tfidf_scores = add_review_to_corpus(input_review, corpus)

similarities = []
for entry_folder in tqdm(os.listdir(base_dir)):
    entry_folder_path = os.path.join(base_dir, entry_folder)
    if not os.path.isdir(entry_folder_path):
        continue

    features_path = os.path.join(entry_folder_path, 'features.pkl')

    review_path = os.path.join(entry_folder_path, 'review.txt')
    review = [line.strip() for line in open(review_path)]

    tfidf_score_path = os.path.join(entry_folder_path, 'tf-idf.pkl')
    with open(tfidf_score_path, 'rb') as f:
        tfidf_scores = pickle.load(f)
    text_similarity = calculate_tfidf_similarity(tfidf_scores, input_tfidf_scores)

    if os.path.exists(features_path):
        with open(features_path, 'rb') as f:
            features_dict = pickle.load(f)

        max_similarity = 0
        for image_name, features in features_dict.items():
            similarity = cosine_similarity(input_normalized_features.reshape(1, -1), features.reshape(1, -1))[0][0]
            img_url_path = os.path.join(entry_folder_path, f"image_url_{image_name.split('_')[1].split('.')[0]}.txt")
            if similarity > max_similarity:
              max_similarity = similarity
              url = [line.strip() for line in open(img_url_path)]
              composite_similarity = (text_similarity + max_similarity)/2

        similarities.append((entry_folder, image_name, max_similarity, url, review, text_similarity, composite_similarity))

similarities.sort(key=lambda x: x[-1], reverse=True)

top_similar_items = similarities[:3]

# plt.figure(figsize=(6, 6))
# plt.subplot(1, 2, 1)
# plt.imshow(input_img)
# plt.title('Original Image')
# plt.axis('off')

# print(input_review)

for idx, (entry_folder, image_name, max_similarity, url, review, text_similarity, composite_similarity) in enumerate(top_similar_items):
    # print(f"Top {idx+1} - Folder: {entry_folder}, Image: {image_name}, Image URL: {url}, Similarity: {similarity}")
    entry_folder_path = os.path.join(base_dir, entry_folder)
    id_path = os.path.join(entry_folder_path, 'id.txt')
    with open(id_path, 'r') as file:
        id = file.read()

    print(f"\nTop {idx+1} - Image URL: {url}")
    print(f"ID: {id}")
    print(f"Review: {review}")
    print(f"Image Similarity: {max_similarity}")
    print(f"Textual Similarity: {text_similarity}")
    print(f"Composite Similarity: {(max_similarity + text_similarity)/2}")